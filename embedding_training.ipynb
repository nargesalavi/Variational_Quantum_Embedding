{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from featuremaps import qaoa, pars_qaoa\n",
    "from fidelity import predict\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "CSWAP = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Settings\n",
    "########################\n",
    "# FIX: For technical reasons, define all kwargs with fixed values (i.e. n_layers=1).\n",
    "# Make sure these are consistent with the variables below.\n",
    "def featmap(*args):\n",
    "    \"\"\"Wrapper for feature map to define specific keyword arguments.\"\"\"\n",
    "    return qaoa(*args, n_layers=4, type_number=2)\n",
    "\n",
    "\n",
    "n_layers = 4  # number of layers for featuremap, if applicable\n",
    "n_inp = 1  # number of wires that feature map acts on\n",
    "n_steps = 300  # steps of GD performed\n",
    "log_step = 5  # how often the test error is calculated\n",
    "batch_size = 1  # how many pairs are sampled in each training step\n",
    "X = np.loadtxt(\"./data/X_1d_sep.txt\", ndmin=2)  # load features\n",
    "Y = np.loadtxt(\"./data/Y_1d_sep.txt\")  # load labels\n",
    "name_output = \"./trained_embeddings/1d_sep-l2-\" + str(n_steps) + \"s-\" + \\\n",
    "              str(n_layers) + \"l-\" + str(n_inp) + \"w\"   # name of output file\n",
    "init_pars = pars_qaoa(n_wires=n_inp, n_layers=n_layers)  # generate initial parameters with helper function\n",
    "pennylane_dev = 'default.qubit'\n",
    "optimizer = qml.RMSPropOptimizer(stepsize=0.01)\n",
    "plot = True\n",
    "save_featmap = True\n",
    "save_plot = True\n",
    "save_intermediate = True  # whether to save feature map in any log_step of training\n",
    "#########################\n",
    "\n",
    "# Use settings to calculate other settings\n",
    "n_all = 2*n_inp + 1\n",
    "dev = qml.device(pennylane_dev, wires=n_all)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, cache=True)\n",
    "def circuit(weights, x1=None, x2=None):\n",
    "\n",
    "    # Load the two inputs into two different registers\n",
    "    featmap(weights, x1, range(1, n_inp+1))\n",
    "    featmap(weights, x2, range(n_inp+1, 2*n_inp+1))\n",
    "\n",
    "    # Do a SWAP test\n",
    "    qml.Hadamard(wires=0)\n",
    "    for k in range(n_inp):\n",
    "        qml.QubitUnitary(CSWAP, wires=[0, k+1, n_inp+k+1])\n",
    "    qml.Hadamard(wires=0)\n",
    "\n",
    "    # Measure overlap by checking ancilla\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "def tr_rr(weights, A=None):\n",
    "    # Compute intra-class overlap A\n",
    "    tr_rr = 0\n",
    "    for a1 in A:\n",
    "        for a2 in A:\n",
    "            tr_rr += circuit(weights, x1=a1, x2=a2)\n",
    "    tr_rr = tr_rr / len(A)**2\n",
    "    return tr_rr\n",
    "\n",
    "\n",
    "def tr_ss(weights, B=None):\n",
    "    # Compute intra-class overlap B\n",
    "    tr_ss = 0\n",
    "    for b1 in B:\n",
    "        for b2 in B:\n",
    "            tr_ss += circuit(weights, x1=b1, x2=b2)\n",
    "    tr_ss = tr_ss/len(B)**2\n",
    "    return tr_ss\n",
    "\n",
    "\n",
    "def tr_rs(weights, A=None, B=None):\n",
    "    # Compute inter-class overlap A-B\n",
    "    tr_rs = 0\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            tr_rs += circuit(weights, x1=a, x2=b)\n",
    "    tr_rs = tr_rs/(len(A)*len(B))\n",
    "    return tr_rs\n",
    "\n",
    "\n",
    "def cost(weights, A=None, B=None):\n",
    "\n",
    "    # Fidelity cost,\n",
    "    rr = tr_rr(weights, A=A)\n",
    "    ss = tr_ss(weights, B=B)\n",
    "    rs = tr_rs(weights, A=A, B=B)\n",
    "    distance = - rs + 0.5 * (ss + rr)\n",
    "    return 1 - distance  # min is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters  [0.001 0.001 0.001 0.001]\n",
      "Initial cost  0  --  0.9936180512296336\n",
      "Step 5 -- rs 0.565048-- rr 0.680598 -- ss 0.642162 -- cst 0.903668 -- time 15.123474\n",
      "Step 10 -- rs 0.489529-- rr 0.675754 -- ss 0.698327 -- cst 0.802488 -- time 30.063501\n",
      "Step 15 -- rs 0.417952-- rr 0.678116 -- ss 0.751909 -- cst 0.702940 -- time 45.003456\n",
      "Step 20 -- rs 0.385273-- rr 0.684276 -- ss 0.776068 -- cst 0.655102 -- time 59.994313\n",
      "Step 25 -- rs 0.334064-- rr 0.690923 -- ss 0.812927 -- cst 0.582139 -- time 74.877746\n",
      "Step 30 -- rs 0.300946-- rr 0.703379 -- ss 0.830350 -- cst 0.534082 -- time 89.994001\n",
      "Step 35 -- rs 0.245151-- rr 0.710973 -- ss 0.874124 -- cst 0.452602 -- time 104.894010\n",
      "Step 40 -- rs 0.219839-- rr 0.712828 -- ss 0.898730 -- cst 0.414060 -- time 119.708139\n",
      "Step 45 -- rs 0.201447-- rr 0.728894 -- ss 0.908482 -- cst 0.382759 -- time 134.600559\n",
      "Step 50 -- rs 0.194111-- rr 0.753896 -- ss 0.896898 -- cst 0.368714 -- time 149.362966\n",
      "Step 55 -- rs 0.174091-- rr 0.753188 -- ss 0.922602 -- cst 0.336196 -- time 164.227168\n",
      "Step 60 -- rs 0.165087-- rr 0.771934 -- ss 0.917807 -- cst 0.320217 -- time 179.208379\n",
      "Step 65 -- rs 0.151990-- rr 0.788005 -- ss 0.923697 -- cst 0.296139 -- time 194.152162\n",
      "Step 70 -- rs 0.145001-- rr 0.804423 -- ss 0.921114 -- cst 0.282232 -- time 210.687486\n",
      "Step 75 -- rs 0.125235-- rr 0.819473 -- ss 0.940867 -- cst 0.245065 -- time 227.503784\n",
      "Step 80 -- rs 0.117364-- rr 0.829635 -- ss 0.946392 -- cst 0.229350 -- time 244.207487\n",
      "Step 85 -- rs 0.109743-- rr 0.840809 -- ss 0.949788 -- cst 0.214445 -- time 260.646723\n",
      "Step 90 -- rs 0.105880-- rr 0.846503 -- ss 0.950417 -- cst 0.207420 -- time 278.499920\n",
      "Step 95 -- rs 0.099221-- rr 0.854236 -- ss 0.956587 -- cst 0.193810 -- time 296.697563\n",
      "Step 100 -- rs 0.095711-- rr 0.858857 -- ss 0.960998 -- cst 0.185783 -- time 311.599366\n",
      "Step 105 -- rs 0.094921-- rr 0.860162 -- ss 0.965193 -- cst 0.182243 -- time 327.377201\n",
      "Step 110 -- rs 0.088411-- rr 0.870115 -- ss 0.964877 -- cst 0.170914 -- time 342.348081\n",
      "Step 115 -- rs 0.082262-- rr 0.878404 -- ss 0.967701 -- cst 0.159209 -- time 357.133779\n",
      "Step 120 -- rs 0.076810-- rr 0.885609 -- ss 0.966454 -- cst 0.150778 -- time 371.956999\n",
      "Step 125 -- rs 0.076212-- rr 0.888243 -- ss 0.971855 -- cst 0.146163 -- time 386.803360\n",
      "Step 130 -- rs 0.071102-- rr 0.894919 -- ss 0.970636 -- cst 0.138325 -- time 401.695664\n",
      "Step 135 -- rs 0.067702-- rr 0.899616 -- ss 0.973999 -- cst 0.130895 -- time 416.535886\n",
      "Step 140 -- rs 0.066231-- rr 0.902287 -- ss 0.974887 -- cst 0.127644 -- time 432.542319\n",
      "Step 145 -- rs 0.067498-- rr 0.906562 -- ss 0.979426 -- cst 0.124504 -- time 447.272174\n",
      "Step 150 -- rs 0.074073-- rr 0.909486 -- ss 0.982766 -- cst 0.127947 -- time 462.851143\n",
      "Step 155 -- rs 0.067406-- rr 0.908930 -- ss 0.981979 -- cst 0.121951 -- time 478.451451\n",
      "Step 160 -- rs 0.069640-- rr 0.910338 -- ss 0.983478 -- cst 0.122731 -- time 493.840979\n",
      "Step 165 -- rs 0.075041-- rr 0.910009 -- ss 0.983755 -- cst 0.128159 -- time 508.627452\n",
      "Step 170 -- rs 0.066673-- rr 0.908458 -- ss 0.981114 -- cst 0.121887 -- time 523.453570\n",
      "Step 175 -- rs 0.061223-- rr 0.907794 -- ss 0.979880 -- cst 0.117386 -- time 538.171944\n",
      "Step 180 -- rs 0.063867-- rr 0.903258 -- ss 0.972366 -- cst 0.126055 -- time 554.491700\n",
      "Step 185 -- rs 0.061950-- rr 0.904794 -- ss 0.974853 -- cst 0.122127 -- time 570.628360\n",
      "Step 190 -- rs 0.061720-- rr 0.904673 -- ss 0.975289 -- cst 0.121739 -- time 587.315954\n",
      "Step 195 -- rs 0.061628-- rr 0.904514 -- ss 0.975639 -- cst 0.121552 -- time 602.710909\n",
      "Step 200 -- rs 0.059690-- rr 0.905583 -- ss 0.978917 -- cst 0.117440 -- time 617.890581\n",
      "Step 205 -- rs 0.058496-- rr 0.909726 -- ss 0.984128 -- cst 0.111569 -- time 634.268243\n",
      "Step 210 -- rs 0.061564-- rr 0.902813 -- ss 0.978307 -- cst 0.121005 -- time 649.462567\n",
      "Step 215 -- rs 0.058291-- rr 0.905746 -- ss 0.981459 -- cst 0.114688 -- time 664.509430\n",
      "Step 220 -- rs 0.056786-- rr 0.907656 -- ss 0.983716 -- cst 0.111100 -- time 679.263373\n",
      "Step 225 -- rs 0.056511-- rr 0.907283 -- ss 0.983960 -- cst 0.110890 -- time 694.395751\n",
      "Step 230 -- rs 0.060048-- rr 0.903651 -- ss 0.981414 -- cst 0.117515 -- time 709.873170\n",
      "Step 235 -- rs 0.055508-- rr 0.909144 -- ss 0.985867 -- cst 0.108003 -- time 724.513443\n",
      "Step 240 -- rs 0.055445-- rr 0.908919 -- ss 0.985369 -- cst 0.108301 -- time 739.322404\n",
      "Step 245 -- rs 0.055843-- rr 0.908302 -- ss 0.984450 -- cst 0.109467 -- time 754.354585\n",
      "Step 250 -- rs 0.056243-- rr 0.912466 -- ss 0.986713 -- cst 0.106653 -- time 770.276547\n",
      "Step 255 -- rs 0.055190-- rr 0.910564 -- ss 0.985936 -- cst 0.106940 -- time 785.914607\n",
      "Step 260 -- rs 0.055801-- rr 0.907531 -- ss 0.984648 -- cst 0.109712 -- time 802.079399\n",
      "Step 265 -- rs 0.059966-- rr 0.904028 -- ss 0.982796 -- cst 0.116554 -- time 817.895294\n",
      "Step 270 -- rs 0.060772-- rr 0.903233 -- ss 0.983044 -- cst 0.117633 -- time 833.839766\n",
      "Step 275 -- rs 0.054013-- rr 0.912175 -- ss 0.987717 -- cst 0.104067 -- time 849.658494\n",
      "Step 280 -- rs 0.054840-- rr 0.907974 -- ss 0.986227 -- cst 0.107739 -- time 865.546922\n",
      "Step 285 -- rs 0.054187-- rr 0.908654 -- ss 0.986767 -- cst 0.106476 -- time 881.380705\n",
      "Step 290 -- rs 0.053259-- rr 0.910475 -- ss 0.987653 -- cst 0.104194 -- time 897.228113\n",
      "Step 295 -- rs 0.053153-- rr 0.910955 -- ss 0.988036 -- cst 0.103657 -- time 913.088406\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Divide inputs into classes\n",
    "A = X[Y == -1]\n",
    "B = X[Y == 1]\n",
    "\n",
    "# Optimise the circuit\n",
    "cst_history = []\n",
    "rr_history = []\n",
    "ss_history = []\n",
    "rs_history = []\n",
    "par_history = [init_pars]\n",
    "pars = init_pars\n",
    "print(\"Initial parameters \", init_pars)\n",
    "cst = cost(pars, A=A, B=B)\n",
    "print(\"Initial cost \", 0, \" -- \", cst)\n",
    "\n",
    "if save_featmap:\n",
    "    featmap_settings = {'pars': pars,\n",
    "                        'step': 0,\n",
    "                        'featmap': pickle.dumps(featmap),  # serialise and save_featmap the feature map\n",
    "                        'n_layers': n_layers,\n",
    "                        'n_wires': n_inp,\n",
    "                        'X': X,\n",
    "                        'Y': Y}\n",
    "    np.save(name_output + \".npy\", featmap_settings)\n",
    "start_time = time.time()\n",
    "for i in range(n_steps):\n",
    "\n",
    "    # Sample a batch of pairs\n",
    "    selectA = np.random.choice(range(len(A)), size=(batch_size,), replace=True)\n",
    "    selectB = np.random.choice(range(len(B)), size=(batch_size,), replace=True)\n",
    "    A_batch = [A[s] for s in selectA]\n",
    "    B_batch = [B[s] for s in selectB]\n",
    "\n",
    "    # Walk one optimization step (using all training samples)\n",
    "    pars = optimizer.step(lambda w: cost(w, A=A_batch, B=B_batch), pars)\n",
    "    par_history.append(pars)\n",
    "\n",
    "    if i % log_step == 0 and i != 0:\n",
    "        cst = cost(pars, A=A, B=B)\n",
    "        rr = tr_rr(pars, A=A)\n",
    "        ss = tr_ss(pars, B=B)\n",
    "        rs = tr_rs(pars, A=A, B=B)\n",
    "        cst_history.append([i, cst])\n",
    "        rr_history.append([i, rr])\n",
    "        ss_history.append([i, ss])\n",
    "        rs_history.append([i, rs])\n",
    "        if save_featmap and save_intermediate:\n",
    "            # Update pars and overwrite\n",
    "            featmap_settings['pars'] = pars\n",
    "            featmap_settings['step'] = i\n",
    "            np.save(name_output + \".npy\", featmap_settings)\n",
    "\n",
    "        print(\"Step {} -- rs {:2f}-- rr {:2f} -- ss {:2f} -- cst {:2f} -- time {:7f}\".\n",
    "              format(i, rs, rr, ss, cst, (time.time() - start_time)))\n",
    "# Update pars and overwrite\n",
    "featmap_settings['pars'] = pars\n",
    "featmap_settings['step'] = i\n",
    "np.save(name_output + \".npy\", featmap_settings)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    import seaborn as sns\n",
    "    sns.set(context='notebook', font='serif')\n",
    "\n",
    "    # Start figure\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    # Plotting 1: original data\n",
    "    ax1 = fig.add_subplot(2, 3, 1)\n",
    "    ax1.set_title(\"Original data\", pad=20)\n",
    "    if len(A[0]) == 2:\n",
    "        ax1.scatter(A[:, 0], A[:, 1], c='r')\n",
    "        ax1.scatter(B[:, 0], B[:, 1], c='b')\n",
    "        ax1.set_ylim((-2, 2))\n",
    "    elif len(A[0]) == 1:\n",
    "        ax1.scatter(A[:, 0], np.zeros(len(A)), c='r')\n",
    "        ax1.scatter(B[:, 0], np.zeros(len(B)), c='b')\n",
    "        ax1.set_ylim((-0.1, 0.1))\n",
    "\n",
    "    ax1.set_xlim((-2, 2))\n",
    "    # Plotting 2: gram matrix in original space\n",
    "    ax5 = fig.add_subplot(2, 3, 4)\n",
    "    ax5.grid(False)\n",
    "    X_normal = [x / len(x) for x in X]\n",
    "    gram_original = [[np.dot(x1, x2) for x1 in X_normal] for x2 in X_normal]\n",
    "    ax5.matshow(gram_original, cmap='Greys')\n",
    "    # Plotting 3: untrained gram matrix in Hilbert space\n",
    "    ax6 = fig.add_subplot(2, 3, 5)\n",
    "    ax6.set_title(\"Step 0\", pad=20)\n",
    "    ax6.grid(False)\n",
    "    gram_before = [[circuit(init_pars, x1=x1, x2=x2) for x1 in X] for x2 in X]\n",
    "    cax1 = ax6.matshow(gram_before, vmin=0, vmax=1)\n",
    "    # Plotting 4: trained gram matrix in Hilbert space\n",
    "    ax7 = fig.add_subplot(2, 3, 6)\n",
    "    ax7.set_title(\"Step \" + str(n_steps), pad=20)\n",
    "    ax7.grid(False)\n",
    "    gram_after = [[circuit(pars, x1=x1, x2=x2) for x1 in X] for x2 in X]\n",
    "    cax2 = ax7.matshow(gram_after, vmin=0, vmax=1)\n",
    "    # Plotting 5: cost\n",
    "    if len(cst_history) > 0:\n",
    "        ax2 = fig.add_subplot(2, 3, 2)\n",
    "        cst_history = np.array(cst_history)\n",
    "        rr_history = np.array(rr_history)\n",
    "        ss_history = np.array(ss_history)\n",
    "        rs_history = np.array(rs_history)\n",
    "        ax2.plot(cst_history[:, 0], cst_history[:, 1],\n",
    "                 color='blue', marker='', linestyle='-', linewidth=2.5, label=\"cost\")\n",
    "        ax2.plot(rr_history[:, 0], rr_history[:, 1],\n",
    "                 color='red', marker='', linestyle='--', linewidth=2.5, label=\"tr rr\")\n",
    "        ax2.plot(ss_history[:, 0], ss_history[:, 1],\n",
    "                 color='red', marker='', linestyle=':', linewidth=2.5, label=\"tr ss\")\n",
    "        ax2.plot(rs_history[:, 0], rs_history[:, 1],\n",
    "                 color='red', marker='', linestyle='-.', linewidth=2.5, label=\"tr rs\")\n",
    "        plt.legend(fancybox=True, framealpha=0.5, loc='lower left')\n",
    "        ax2.set_ylim((0, 1))\n",
    "        ax2.set_xlabel(\"steps\")\n",
    "\n",
    "\n",
    "    # Plotting 6\n",
    "    if len(cst_history) > 0:\n",
    "        ax3 = fig.add_subplot(2, 3, 3)\n",
    "        par_history = np.array(par_history)\n",
    "        for i in range(len(par_history[0])):\n",
    "            ax3.plot(range(len(par_history)), par_history[:, i], linewidth=2.0, label=\"parameters\")\n",
    "        ax3.set_xlabel(\"epochs\")\n",
    "        # fake line for legend\n",
    "        ax3.plot([], linewidth=2.0, label=\"parameters\")\n",
    "\n",
    "    fig.colorbar(cax1, orientation=\"vertical\", pad=0.2)\n",
    "    fig.colorbar(cax2, orientation=\"vertical\", pad=0.2)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "    plt.legend(fancybox=True, framealpha=0.5, loc='lower left')\n",
    "\n",
    "    if save_plot:\n",
    "        plt.savefig(name_output + \".svg\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"./data/X_1d_sep_test.txt\", ndmin=2)  # load features\n",
    "Y = np.loadtxt(\"./data/Y_1d_sep_test.txt\") \n",
    "path_to_featmap = \"./trained_embeddings/1d_sep-l2-300s-4l-1w.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(X,Y):\n",
    "    predicted_y = predict(x, path_to_featmap, n_samples=None,\n",
    "            probs_A=None, probs_B=None, binary=True, implementation=\"exact\", seed=None)\n",
    "    print('y:', y, ' predicted y: ', predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
